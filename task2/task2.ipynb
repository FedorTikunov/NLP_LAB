{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc04391f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f7d9255",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fedor\\anaconda3\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading builder script: 100%|█████████████████████████████████████████████████████████| 9.57k/9.57k [00:00<?, ?B/s]\n",
      "Downloading metadata: 100%|███████████████████████████████████████████████████████████████| 3.73k/3.73k [00:00<?, ?B/s]\n",
      "Downloading readme: 100%|█████████████████████████████████████████████████████████████████| 12.3k/12.3k [00:00<?, ?B/s]\n",
      "Downloading data: 100%|██████████████████████████████████████████████████████████████| 983k/983k [00:01<00:00, 856kB/s]\n",
      "Generating train split: 100%|███████████████████████████████████████████| 14041/14041 [00:01<00:00, 9789.68 examples/s]\n",
      "Generating validation split: 100%|████████████████████████████████████████| 3250/3250 [00:00<00:00, 8976.28 examples/s]\n",
      "Generating test split: 100%|██████████████████████████████████████████████| 3453/3453 [00:00<00:00, 9776.00 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the CoNLL 2003 dataset\n",
    "dataset = load_dataset('conll2003')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f277254e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a FastText model\n",
    "fastmodel = FastText(dataset['train']['tokens'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "a7f641cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "\n",
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Create an initial hidden state of zeros\n",
    "        h0 = torch.zeros(1, x.size(0), self.hidden_size).to(x.device) \n",
    "        out, _ = self.rnn(x.unsqueeze(1), h0)\n",
    "        out = self.fc(out.squeeze(1))\n",
    "        return out\n",
    "\n",
    "flat_labels = [label for sublist in dataset['train']['ner_tags'] for label in sublist]\n",
    "\n",
    "# Get the unique ner_tags\n",
    "unique_labels = set(flat_labels)\n",
    "# Initialize the model\n",
    "model = SimpleRNN(input_size=100, hidden_size=32, output_size=len(unique_labels))\n",
    "\n",
    "# Define a loss function and an optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Convert words to embeddings\n",
    "inputs_train = [fastmodel.wv[word] for word in dataset['train']['tokens']]\n",
    "inputs_train = [torch.tensor(sentence).float() for sentence in inputs_train]\n",
    "labels_train = dataset['train']['ner_tags']\n",
    "labels_train = [torch.tensor(label) for label in labels_train]\n",
    "\n",
    "\n",
    "inputs_valid = [fastmodel.wv[word] for word in dataset['validation']['tokens']]\n",
    "inputs_valid = [torch.tensor(sentence).float() for sentence in inputs_valid]\n",
    "labels_valid = dataset['validation']['ner_tags']\n",
    "labels_valid = [torch.tensor(label) for label in labels_valid]\n",
    "\n",
    "inputs_test = [fastmodel.wv[word] for word in dataset['test']['tokens']]\n",
    "inputs_test = [torch.tensor(sentence).float() for sentence in inputs_test]\n",
    "labels_test = dataset['test']['ner_tags']\n",
    "labels_test = [torch.tensor(label) for label in labels_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "771f0ed3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/100, Loss: 0.8844480514526367\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.95      0.94     42759\n",
      "           1       0.30      0.26      0.28      1842\n",
      "           2       0.14      0.52      0.22      1307\n",
      "           3       0.33      0.02      0.04      1341\n",
      "           4       1.00      0.00      0.00       751\n",
      "           5       0.59      0.32      0.41      1837\n",
      "           6       1.00      0.00      0.00       257\n",
      "           7       0.85      0.02      0.05       922\n",
      "           8       1.00      0.00      0.00       346\n",
      "\n",
      "    accuracy                           0.82     51362\n",
      "   macro avg       0.68      0.23      0.22     51362\n",
      "weighted avg       0.86      0.82      0.81     51362\n",
      "\n",
      "Epoch 20/100, Loss: 0.9995336532592773\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.95      0.94     42759\n",
      "           1       0.30      0.27      0.29      1842\n",
      "           2       0.15      0.56      0.24      1307\n",
      "           3       0.35      0.02      0.04      1341\n",
      "           4       0.10      0.00      0.00       751\n",
      "           5       0.62      0.33      0.43      1837\n",
      "           6       0.69      0.07      0.13       257\n",
      "           7       0.00      0.00      1.00       922\n",
      "           8       1.00      0.00      0.00       346\n",
      "\n",
      "    accuracy                           0.83     51362\n",
      "   macro avg       0.46      0.25      0.34     51362\n",
      "weighted avg       0.83      0.83      0.84     51362\n",
      "\n",
      "Epoch 30/100, Loss: 1.1394615173339844\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.96      0.94     42759\n",
      "           1       0.32      0.31      0.32      1842\n",
      "           2       0.15      0.53      0.24      1307\n",
      "           3       0.23      0.02      0.04      1341\n",
      "           4       0.03      0.00      0.00       751\n",
      "           5       0.62      0.34      0.44      1837\n",
      "           6       0.69      0.07      0.13       257\n",
      "           7       0.85      0.02      0.05       922\n",
      "           8       1.00      0.00      0.00       346\n",
      "\n",
      "    accuracy                           0.83     51362\n",
      "   macro avg       0.54      0.25      0.24     51362\n",
      "weighted avg       0.85      0.83      0.82     51362\n",
      "\n",
      "Epoch 40/100, Loss: 1.063141107559204\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.96      0.95     42759\n",
      "           1       0.31      0.28      0.29      1842\n",
      "           2       0.15      0.54      0.23      1307\n",
      "           3       0.28      0.04      0.08      1341\n",
      "           4       0.24      0.02      0.04       751\n",
      "           5       0.67      0.34      0.45      1837\n",
      "           6       0.69      0.07      0.13       257\n",
      "           7       0.81      0.02      0.05       922\n",
      "           8       1.00      0.00      0.00       346\n",
      "\n",
      "    accuracy                           0.83     51362\n",
      "   macro avg       0.57      0.25      0.25     51362\n",
      "weighted avg       0.85      0.83      0.82     51362\n",
      "\n",
      "Epoch 50/100, Loss: 1.047203540802002\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.96      0.95     42759\n",
      "           1       0.32      0.30      0.31      1842\n",
      "           2       0.16      0.54      0.24      1307\n",
      "           3       0.25      0.05      0.08      1341\n",
      "           4       0.17      0.02      0.04       751\n",
      "           5       0.68      0.36      0.47      1837\n",
      "           6       0.69      0.07      0.13       257\n",
      "           7       0.58      0.02      0.05       922\n",
      "           8       1.00      0.00      0.00       346\n",
      "\n",
      "    accuracy                           0.84     51362\n",
      "   macro avg       0.53      0.26      0.25     51362\n",
      "weighted avg       0.85      0.84      0.83     51362\n",
      "\n",
      "Epoch 60/100, Loss: 1.0592267513275146\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.96      0.95     42759\n",
      "           1       0.32      0.30      0.31      1842\n",
      "           2       0.15      0.54      0.24      1307\n",
      "           3       0.29      0.06      0.09      1341\n",
      "           4       0.24      0.03      0.05       751\n",
      "           5       0.68      0.36      0.47      1837\n",
      "           6       0.69      0.07      0.13       257\n",
      "           7       0.49      0.05      0.09       922\n",
      "           8       1.00      0.00      0.00       346\n",
      "\n",
      "    accuracy                           0.84     51362\n",
      "   macro avg       0.53      0.26      0.26     51362\n",
      "weighted avg       0.85      0.84      0.83     51362\n",
      "\n",
      "Epoch 70/100, Loss: 1.060288429260254\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.95      0.95     42759\n",
      "           1       0.32      0.30      0.31      1842\n",
      "           2       0.15      0.56      0.24      1307\n",
      "           3       0.30      0.06      0.10      1341\n",
      "           4       0.24      0.03      0.05       751\n",
      "           5       0.68      0.36      0.47      1837\n",
      "           6       0.69      0.07      0.13       257\n",
      "           7       0.45      0.07      0.11       922\n",
      "           8       0.81      0.04      0.07       346\n",
      "\n",
      "    accuracy                           0.84     51362\n",
      "   macro avg       0.51      0.27      0.27     51362\n",
      "weighted avg       0.85      0.84      0.83     51362\n",
      "\n",
      "Epoch 80/100, Loss: 1.0622010231018066\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.95      0.95     42759\n",
      "           1       0.33      0.30      0.32      1842\n",
      "           2       0.15      0.57      0.24      1307\n",
      "           3       0.31      0.06      0.10      1341\n",
      "           4       0.27      0.03      0.05       751\n",
      "           5       0.66      0.36      0.46      1837\n",
      "           6       0.69      0.07      0.13       257\n",
      "           7       0.40      0.06      0.11       922\n",
      "           8       0.81      0.04      0.07       346\n",
      "\n",
      "    accuracy                           0.84     51362\n",
      "   macro avg       0.51      0.27      0.27     51362\n",
      "weighted avg       0.85      0.84      0.83     51362\n",
      "\n",
      "Epoch 90/100, Loss: 1.079347014427185\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.95      0.95     42759\n",
      "           1       0.33      0.29      0.31      1842\n",
      "           2       0.15      0.59      0.24      1307\n",
      "           3       0.33      0.07      0.11      1341\n",
      "           4       0.25      0.03      0.05       751\n",
      "           5       0.65      0.36      0.47      1837\n",
      "           6       0.67      0.07      0.13       257\n",
      "           7       0.39      0.06      0.11       922\n",
      "           8       0.81      0.04      0.07       346\n",
      "\n",
      "    accuracy                           0.83     51362\n",
      "   macro avg       0.50      0.27      0.27     51362\n",
      "weighted avg       0.85      0.83      0.83     51362\n",
      "\n",
      "Epoch 100/100, Loss: 1.0954937934875488\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.95      0.95     42759\n",
      "           1       0.33      0.29      0.31      1842\n",
      "           2       0.15      0.59      0.24      1307\n",
      "           3       0.32      0.08      0.13      1341\n",
      "           4       0.23      0.02      0.04       751\n",
      "           5       0.66      0.36      0.47      1837\n",
      "           6       0.67      0.07      0.13       257\n",
      "           7       0.43      0.07      0.11       922\n",
      "           8       0.81      0.04      0.07       346\n",
      "\n",
      "    accuracy                           0.84     51362\n",
      "   macro avg       0.51      0.27      0.27     51362\n",
      "weighted avg       0.85      0.84      0.83     51362\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(100):  # number of epochs\n",
    "    true_labels = []\n",
    "    pred_labels = []\n",
    "    for i in range(len(inputs_train)):  # number of sentences in the dataset\n",
    "        # Forward pass\n",
    "        outputs = model.forward(inputs_train[i])\n",
    "        \n",
    "        # Calculate the loss\n",
    "        loss = criterion(outputs, labels_train[i])\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch {epoch+1}/{100}, Loss: {loss.item()}')\n",
    "        true_labels = []\n",
    "        pred_labels = []\n",
    "        for i in range(len(inputs_valid)):  # number of sentences in the dataset\n",
    "            # Forward pass\n",
    "            outputs = model.forward(inputs_valid[i])\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            # Add the true and predicted labels to their respective lists\n",
    "            true_labels.append(labels_valid[i].tolist())\n",
    "            pred_labels.append(predicted.tolist())\n",
    "\n",
    "        true_labels_flat = [label for sublist in true_labels for label in sublist]\n",
    "        pred_labels_flat = [label for sublist in pred_labels for label in sublist]\n",
    "        \n",
    "        print(classification_report(true_labels_flat, pred_labels_flat, zero_division=1))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "6f65a7e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.95      0.94     38323\n",
      "           1       0.31      0.29      0.30      1617\n",
      "           2       0.15      0.61      0.24      1156\n",
      "           3       0.42      0.09      0.15      1661\n",
      "           4       0.33      0.05      0.08       835\n",
      "           5       0.69      0.39      0.50      1668\n",
      "           6       0.86      0.07      0.14       257\n",
      "           7       0.44      0.10      0.17       702\n",
      "           8       0.19      0.02      0.03       216\n",
      "\n",
      "    accuracy                           0.83     46435\n",
      "   macro avg       0.48      0.29      0.28     46435\n",
      "weighted avg       0.85      0.83      0.82     46435\n",
      "\n"
     ]
    }
   ],
   "source": [
    "true_labels = []\n",
    "pred_labels = []\n",
    "for i in range(len(inputs_test)):  # number of sentences in the dataset\n",
    "    # Forward pass\n",
    "    outputs = model.forward(inputs_test[i])\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    # Add the true and predicted labels to their respective lists\n",
    "    true_labels.append(labels_test[i].tolist())\n",
    "    pred_labels.append(predicted.tolist())\n",
    "            \n",
    "true_labels_flat = [label for sublist in true_labels for label in sublist]\n",
    "pred_labels_flat = [label for sublist in pred_labels for label in sublist]\n",
    "        \n",
    "print(classification_report(true_labels_flat, pred_labels_flat, zero_division=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ca9c74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
